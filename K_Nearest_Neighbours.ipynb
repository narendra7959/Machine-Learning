{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AIjOj1svOIwk"
      },
      "outputs": [],
      "source": [
        "import requests  # Import requests module to download data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the dataset\n",
        "url = \"https://goo.gl/BDYgh5\""
      ],
      "metadata": {
        "id": "CoqJ931UO_vt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset from the given URL\n",
        "response = requests.get(url)\n",
        "dataset_content = response.text  # Read content of the dataset"
      ],
      "metadata": {
        "id": "NypYE0UYPF_4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the dataset to a CSV file\n",
        "filename = \"abalone.csv\"\n",
        "with open(filename, \"w\") as file:\n",
        "    file.write(dataset_content)\n",
        "\n",
        "print(f\"Dataset downloaded and saved as {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaggMKAvPJeU",
        "outputId": "963afcaf-3f38-4c1b-ccbc-c401daffc729"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded and saved as abalone.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import reader  # Import the CSV reader module to read CSV files\n",
        "\n",
        "def load_csv(filename):\n",
        "    \"\"\"Load a CSV file into a dataset (list of lists)\"\"\"\n",
        "\n",
        "    dataset = []  # Initialize an empty list to store dataset rows\n",
        "\n",
        "    with open(filename, 'r') as file:  # Open the specified file in read mode\n",
        "        csv_reader = reader(file)  # Create a CSV reader object to read the file\n",
        "\n",
        "        for row in csv_reader:  # Iterate over each row in the CSV file\n",
        "            if not row:  # Skip empty rows to avoid processing blank lines\n",
        "                continue\n",
        "            dataset.append(row)  # Append the non-empty row to the dataset list\n",
        "\n",
        "    return dataset  # Return the final dataset as a list of lists\n"
      ],
      "metadata": {
        "id": "2ckyxosPQC3K"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_column_to_float(dataset, column):\n",
        "    \"\"\"Convert a column in the dataset from string to float.\"\"\"\n",
        "\n",
        "    for row in dataset:  # Iterate through each row in the dataset\n",
        "        row[column] = float(row[column].strip())  # Strip whitespace and convert the value to float\n"
      ],
      "metadata": {
        "id": "LXYrhrT-QVuz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_column_to_int(dataset, column):\n",
        "    \"\"\"Convert a categorical column in the dataset to integer values.\"\"\"\n",
        "\n",
        "    # Extract all values from the specified column\n",
        "    class_values = [row[column] for row in dataset]\n",
        "\n",
        "    # Get unique values from the column (to assign unique integers)\n",
        "    unique = set(class_values)\n",
        "\n",
        "    # Create a dictionary to map each unique category to an integer\n",
        "    lookup = {value: i for i, value in enumerate(unique)}\n",
        "\n",
        "    # Replace categorical values in the dataset with their corresponding integer values\n",
        "    for row in dataset:\n",
        "        row[column] = lookup[row[column]]\n",
        "\n",
        "    # Return the mapping dictionary for reference\n",
        "    return lookup\n"
      ],
      "metadata": {
        "id": "IqmyeySoQiqi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_minmax(dataset):\n",
        "    \"\"\"Calculate the minimum and maximum values for each column in the dataset.\"\"\"\n",
        "\n",
        "    # Transpose the dataset using zip(*dataset) to get columns instead of rows\n",
        "    # Find the min and max for each column and store them in a list\n",
        "    minmax = [[min(column), max(column)] for column in zip(*dataset)]\n",
        "\n",
        "    return minmax  # Return the list of [min, max] pairs for each column\n",
        "\n",
        "def normalize_dataset(dataset, minmax):\n",
        "    \"\"\"Normalize the dataset by scaling values between 0 and 1 using min-max scaling.\"\"\"\n",
        "\n",
        "    for row in dataset:  # Iterate through each row in the dataset\n",
        "        for i in range(len(row)):  # Iterate through each column in the row\n",
        "            # Apply min-max normalization formula: (value - min) / (max - min)\n",
        "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n"
      ],
      "metadata": {
        "id": "1p2G2nFpQoVb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange  # Import randrange to randomly select indices\n",
        "\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    \"\"\"Split dataset into k folds for cross-validation.\"\"\"\n",
        "\n",
        "    dataset_split = []  # List to store the resulting folds\n",
        "    dataset_copy = list(dataset)  # Create a copy of the dataset to modify safely\n",
        "    fold_size = int(len(dataset) / n_folds)  # Determine the size of each fold\n",
        "\n",
        "    for _ in range(n_folds):  # Repeat for the number of folds\n",
        "        fold = []  # Initialize an empty list for the current fold\n",
        "\n",
        "        while len(fold) < fold_size:  # Keep adding data until fold reaches the required size\n",
        "            index = randrange(len(dataset_copy))  # Randomly select an index from the remaining dataset\n",
        "            fold.append(dataset_copy.pop(index))  # Remove the selected row and add it to the fold\n",
        "\n",
        "        dataset_split.append(fold)  # Add the completed fold to the list of folds\n",
        "\n",
        "    return dataset_split  # Return the list of dataset folds\n"
      ],
      "metadata": {
        "id": "SRfHZKh1Q2ug"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt  # Import the sqrt function to compute square roots\n",
        "\n",
        "def euclidean_distance(row1, row2):\n",
        "    \"\"\"Calculate the Euclidean distance between two data points (rows).\"\"\"\n",
        "\n",
        "    return sqrt(sum((row1[i] - row2[i]) ** 2 for i in range(len(row1) - 1)))\n",
        "    # 1. Subtract corresponding elements in row1 and row2.\n",
        "    # 2. Square the differences.\n",
        "    # 3. Sum up all squared differences.\n",
        "    # 4. Take the square root to get the final Euclidean distance.\n"
      ],
      "metadata": {
        "id": "UmT6g04-Q9Ib"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_neighbors(train, test_row, num_neighbors):\n",
        "    \"\"\"Find the k nearest neighbors of a test row in the training dataset.\"\"\"\n",
        "\n",
        "    # Compute the Euclidean distance between the test_row and every row in the training dataset\n",
        "    distances = [(train_row, euclidean_distance(test_row, train_row)) for train_row in train]\n",
        "\n",
        "    # Sort the list of (train_row, distance) tuples based on the distance (ascending order)\n",
        "    distances.sort(key=lambda tup: tup[1])\n",
        "\n",
        "    # Extract the first 'num_neighbors' rows (i.e., the closest neighbors)\n",
        "    return [distances[i][0] for i in range(num_neighbors)]\n"
      ],
      "metadata": {
        "id": "tQRaDcYdRIkc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_classification(train, test_row, num_neighbors):\n",
        "    \"\"\"Predict the class label for a given test row using the k-Nearest Neighbors algorithm.\"\"\"\n",
        "\n",
        "    # Find the k nearest neighbors of the test_row in the training dataset\n",
        "    neighbors = get_neighbors(train, test_row, num_neighbors)\n",
        "\n",
        "    # Extract the class labels (last column) from the k nearest neighbors\n",
        "    output_values = [row[-1] for row in neighbors]\n",
        "\n",
        "    # Return the most common class label among the neighbors\n",
        "    return max(set(output_values), key=output_values.count)\n"
      ],
      "metadata": {
        "id": "IjvyiaIiRNmU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_nearest_neighbors(train, test, num_neighbors):\n",
        "    \"\"\"Make predictions for each test instance using the k-Nearest Neighbors algorithm.\"\"\"\n",
        "\n",
        "    # Iterate over each test row and predict its class using k-NN\n",
        "    return [predict_classification(train, row, num_neighbors) for row in test]\n"
      ],
      "metadata": {
        "id": "J4m-J7hORU-F"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_metric(actual, predicted):\n",
        "    \"\"\"Calculate the accuracy percentage of predictions compared to actual values.\"\"\"\n",
        "\n",
        "    # Count the number of correct predictions (where actual == predicted)\n",
        "    correct = sum(1 for i in range(len(actual)) if actual[i] == predicted[i])\n",
        "\n",
        "    # Compute accuracy as (correct predictions / total predictions) * 100\n",
        "    return (correct / float(len(actual))) * 100.0\n"
      ],
      "metadata": {
        "id": "yhc8T4FURZ5l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    \"\"\"Evaluate a machine learning algorithm using k-fold cross-validation.\"\"\"\n",
        "\n",
        "    # Split the dataset into k folds\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "\n",
        "    scores = []  # List to store accuracy scores for each fold\n",
        "\n",
        "    for fold in folds:  # Iterate over each fold\n",
        "\n",
        "        # Create the training set by combining all folds except the current one\n",
        "        train_set = sum([f for f in folds if f != fold], [])\n",
        "\n",
        "        # Create the test set as a copy of the current fold\n",
        "        test_set = [row[:] for row in fold]\n",
        "\n",
        "        # Remove class labels from test set (simulating unknown predictions)\n",
        "        for row in test_set:\n",
        "            row[-1] = None  # Setting the last column (class label) to None\n",
        "\n",
        "        # Run the provided algorithm to get predictions\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "\n",
        "        # Extract actual class labels from the original fold\n",
        "        actual = [row[-1] for row in fold]\n",
        "\n",
        "        # Compute accuracy of predictions\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "\n",
        "        # Store the accuracy score\n",
        "        scores.append(accuracy)\n",
        "\n",
        "    return scores  # Return the list of accuracy scores for each fold\n"
      ],
      "metadata": {
        "id": "24_wvyLgRgTF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import seed  # Import seed function to control randomness\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "seed(1)\n",
        "\n",
        "# Define the dataset file name\n",
        "filename = \"abalone.csv\"\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "dataset = load_csv(filename)\n",
        "\n",
        "# Convert all columns (except the first) from strings to floating-point numbers\n",
        "for i in range(1, len(dataset[0])):\n",
        "    str_column_to_float(dataset, i)\n",
        "\n",
        "# Convert the first column (categorical feature) to integer values\n",
        "str_column_to_int(dataset, 0)\n",
        "\n",
        "# Define parameters for k-fold cross-validation and k-NN\n",
        "n_folds = 5  # Number of folds for cross-validation\n",
        "num_neighbors = 5  # Number of neighbors for k-NN\n",
        "\n",
        "# Evaluate the k-Nearest Neighbors algorithm using cross-validation\n",
        "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, num_neighbors)\n",
        "\n",
        "# Print the accuracy scores for each fold\n",
        "print('Scores:', scores)\n",
        "\n",
        "# Calculate and print the mean accuracy across all folds\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whrLNliXRmm4",
        "outputId": "d7819c29-f038-49dd-e0e8-ba2589ed3bfe"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [24.790419161676645, 21.79640718562874, 23.592814371257482, 21.676646706586826, 23.353293413173652]\n",
            "Mean Accuracy: 23.042%\n"
          ]
        }
      ]
    }
  ]
}